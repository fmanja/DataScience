{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "<h1>F. Manja - Exercise 1 Submission</h1>\nWelcome to exercise one of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll apply the basics of functional and parallel programming.\n\nLet\u2019s start with a simple example. Let\u2019s consider you have a list of integers.\n\nLet\u2019s find out what the size of this list is.\n\nNote that we already provide an RDD object, so please have a look at the RDD API in order to find out what function to use: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n\nThe following link contains additional documentation: https://spark.apache.org/docs/latest/rdd-programming-guide.html"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2>Apache Spark Setup</h2>\nApache Spark (Mac install)\nDon't forget to start the Spark Shell with this command> spark-shell"}, {"metadata": {}, "cell_type": "markdown", "source": "<h3>(Optional)</h3>\nInstall findspark and pyspark"}, {"metadata": {}, "cell_type": "code", "source": "# Uncomment lines below if needed\n#! pip install findspark \n! pip install pyspark", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Collecting pyspark\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215.7MB 165kB/s  eta 0:00:01   |\u2588                               | 7.1MB 16.8MB/s eta 0:00:13     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f            | 128.9MB 46.9MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 38.6MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.4\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark import SparkContext\nfrom pyspark import SparkConf", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext(appName=\"MyFirstApp\")", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(range(100))", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd.count()", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "100"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.sum()", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "4950"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}