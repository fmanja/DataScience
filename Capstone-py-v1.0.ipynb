{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Capstone - ACME Startup Relocation in New York City</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction/Problem\n",
    "\n",
    "ACME Start is a Machine Learning (ML) startup in New York City that recently received Series A Funding in the amount of $20M USD. ACME currently has 10 employees and is looking to grow to 20 employees by years end.  The startup has outgrown its current location and needs to relocate to accommodate current and future employees.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The Chief Executive Officer (CEO) is concerned that the relocation may result in staff attrition so the CEO asks the Human Resources (HR) Director to perform a survey of employees. The survey asks employees to rank factors that the company will incorporate in the selection of the new location. These factors include proximity to mass resturants, parks, and gyms. The HR Director will use these results of the survey and Foursquare location data to prepare a report of suitable locations for the relocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "<img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DS0103EN/labs/images/lab1_fig2_datascience_methodology_flowchart.png\" width = 500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "The HR Director is the lead for this analysis.  She understands that office relocations can be stressful for employees and at time bring about unwanted attrition so she will prioritize employee engagement.\n",
    "\n",
    "### Analytics Approach\n",
    "The HR Director has asked the Data Science team to conduct a survey to of employees to gauge employee sentiment and use the information learned in that survey to guide the data analysis.\n",
    "\n",
    "### Data Requirements\n",
    "The Data Science team will center their analysis on three data sources:\n",
    "- Employee Survey\n",
    "- New York City Maps\n",
    "- Foursquare Location Data\n",
    "\n",
    "### Data Collection\n",
    "The Data Science team has collected the raw survey results from employees using SurveyMonkey.  The team downloaded mapping files from the NY city website.  The team signed up for a Foursquare account to access data using the Foursquare API.\n",
    "\n",
    "### Data Understanding\n",
    "The Data Science team will use standard data exploration techniques on each data source to learn the structure and dimensions of all data.\n",
    "\n",
    "### Data Preparation\n",
    "The Data Science team will transform data, making sure their are no missing data values, and incorrect data types.  The team will impute missing data from available data. Also, the team will map Foursquare Venue Categories to the ranked features that employees called for in the survey. All data transformation steps will be documented in the Notebook.  \n",
    "\n",
    "### Modeling\n",
    "The Data Science team will furnish maps of potential sites and segment the sites by their features. The team decided to use clustering and segementation techniques as they perform well on geographical data where unsupervised learning is needed. The team will use K-means with K=5.\n",
    "\n",
    "### Evaluation\n",
    "The Data Science team will evaluate sites for suitability by determining what venues are within 200 meters of potential locations.  \n",
    "\n",
    "### Deployment\n",
    "The Data Science team will present findings to the HR Director and CEO.  \n",
    "\n",
    "### Feedback\n",
    "The Data Science team will solicit feedback from the HR Director and CEO, both of whom may ask for additional factors to be included in the analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item1\">Download and Explore Datasets</a>\n",
    "\n",
    "2. <a href=\"#item2\">Explore Neighborhoods in New York City</a>\n",
    "\n",
    "3. <a href=\"#item3\">Analyze Each Neighborhood</a>\n",
    "\n",
    "4. <a href=\"#item4\">Cluster Neighborhoods</a>\n",
    "\n",
    "5. <a href=\"#item5\">Examine Clusters</a>    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get the data and start exploring it, let's download all the dependencies that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \r\n",
      "Warning: >10 possible package resolutions (only showing differing packages):\r\n",
      "  - https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/conda-verify-2.0.0-py36he837df3_0.tar.bz2::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  - defaults::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/conda-verify-2.0.0-py36he837df3_0.tar.bz2::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  - defaults::navigator-updater-0.1.0-py36h7aee5fb_0, https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/conda-verify-2.0.0-py36he837df3_0.tar.bz2::conda-verify-2.0.0-py36he837df3_0\r\n",
      "  - defaults::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, defaults::navigator-updater-0.1.0-py36h7aee5fb_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/conda-verify-2.0.0-py36he837df3_0.tar.bz2::conda-verify-2.0.0-py36he837df3_0\r\n",
      "  - defaults::conda-verify-2.0.0-py36he837df3_0, defaults::navigator-updater-0.1.0-py36h7aee5fb_0, https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0\r\n",
      "  - defaults::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, defaults::conda-verify-2.0.0-py36he837df3_0, defaults::navigator-updater-0.1.0-py36h7aee5fb_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0\r\n",
      "  - defaults::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  - defaults::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, defaults::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/conda-build-3.0.27-py36hb78d8cd_0.tar.bz2::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  - defaults::conda-build-3.0.27-py36hb78d8cd_0, defaults::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  - defaults::conda-build-3.0.27-py36hb78d8cd_0, https://repo.continuum.io/pkgs/main/osx-64/_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0.tar.bz2::_ipyw_jlab_nb_ext_conf-0.1.0-py36h2fc01ae_0, https://repo.continuum.io/pkgs/main/osx-64/conda-verify-2.0.0-py36he837df3_0.tar.bz2::conda-verify-2.0.0-py36he837df3_0, https://repo.continuum.io/pkgs/main/osx-64/navigator-updater-0.1.0-py36h7aee5fb_0.tar.bz2::navigator-updater-0.1.0-py36h7aee5fb_0\r\n",
      "  ... and others\b\b/ \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /anaconda3\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - geopy\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    conda-4.7.12               |           py36_0         3.0 MB  conda-forge\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:         3.0 MB\r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  ca-certificates    pkgs/main::ca-certificates-2019.8.28-0 --> conda-forge::ca-certificates-2019.9.11-hecc5488_0\r\n",
      "\r\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\r\n",
      "\r\n",
      "  certifi               pkgs/main::certifi-2019.9.11-py36_0 --> conda-forge::certifi-2019.6.16-py36_1\r\n",
      "  conda                                           pkgs/main --> conda-forge\r\n",
      "  openssl              pkgs/main::openssl-1.0.2t-h1de35cc_1 --> conda-forge::openssl-1.0.2r-h1de35cc_0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "\r",
      "conda-4.7.12         | 3.0 MB    |                                       |   0% \r",
      "conda-4.7.12         | 3.0 MB    | 1                                     |   1% \r",
      "conda-4.7.12         | 3.0 MB    | ##############1                       |  38% \r",
      "conda-4.7.12         | 3.0 MB    | ##################################### | 100% \r\n",
      "Preparing transaction: \\ \b\bdone\r\n",
      "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Matplotlib version:  2.1.0\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.style.use('ggplot') # optional: for ggplot-like style\n",
    "\n",
    "# check for latest version of Matplotlib\n",
    "print('Matplotlib version: ', mpl.__version__) # >= 2.0.0\n",
    "\n",
    "import re \n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Explore Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey Dataset\n",
    "\n",
    "The Chief Executive Officer (CEO) is concerned that the relocation may result in staff attrition so the CEO asks the Human Resources (HR) Director to perform a survey of employees. The survey asks employees to rank factors that the company will incorporate in the selection of the new location. Some of these factors include proximity to mass transit, childcare, parks, and gyms. The HR Director will use these results of the survey and Foursquare location data to prepare a report of suitable locations for the relocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'ACME_Startup_Survey.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bf8835903661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_survey_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ACME_Startup_Survey.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'ACME_Startup_Survey.csv' does not exist"
     ]
    }
   ],
   "source": [
    "raw_survey_df = pd.read_csv('ACME_Startup_Survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_survey_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix data issue with empty data not identified with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_survey_df.replace(\" \", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_survey_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the data type of Gyms to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_survey_df['Gyms'] = raw_survey_df['Gyms'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgResturants = raw_survey_df['Resturants'].astype(\"float\").mean(axis=0)\n",
    "raw_survey_df[\"Resturants\"].replace(np.nan, avgResturants, inplace=True)\n",
    "print('Set missing Resturants to Average Resturants:',avgResturants)\n",
    "\n",
    "avgGyms = raw_survey_df['Gyms'].astype(\"float\").mean(axis=0)\n",
    "raw_survey_df[\"Gyms\"].replace(np.nan, avgGyms, inplace=True)\n",
    "print('Set missing Gyms to Average Gyms:',avgGyms)\n",
    "\n",
    "avgParks = raw_survey_df['Parks'].astype(\"float\").mean(axis=0)\n",
    "raw_survey_df[\"Parks\"].replace(np.nan, avgParks, inplace=True)\n",
    "print('Set missing Parks to Average Parks:',avgParks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved transformed data as survey dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = raw_survey_df\n",
    "survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 'Exploratory Data Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[['Resturants','Gyms','Parks']].plot(kind='box', figsize=(10, 7))\n",
    "\n",
    "plt.title('Box plots of Survey Results')\n",
    "plt.xlabel('Survey Responses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the means to determine the ranking of factors that staff want to inform the decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df=survey_df[['Resturants','Gyms','Parks']].mean(axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.columns=['Factor','Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df['Rank']=factors_df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the rank order of factors in order of importance to the employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood Dataset\n",
    "\n",
    "Neighborhood has a total of 5 boroughs and 306 neighborhoods. In order to segement the neighborhoods and explore them, we will essentially need a dataset that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the the latitude and logitude coordinates of each neighborhood. \n",
    "\n",
    "Luckily, this dataset exists for free on the web. Feel free to try to find this dataset on your own, but here is the link to the dataset: https://geo.nyu.edu/catalog/nyu_2451_34572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, I downloaded the files and placed it on the server, so you can simply run a `wget` command and access the data. So let's go ahead and do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -q -O 'newyork_data.json' https://cocl.us/new_york_dataset\n",
    "print('Data downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open('newyork_data.json') as json_data:\n",
    "    newyork_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how all the relevant data is in the *features* key, which is basically a list of the neighborhoods. So, let's define a new variable that includes this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_data = newyork_data['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first item in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "neighborhoods_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranform the data into a *pandas* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is essentially transforming this data of nested Python dictionaries into a *pandas* dataframe. So let's start by creating an empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataframe columns\n",
    "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
    "\n",
    "# instantiate the dataframe\n",
    "neighborhoods = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the empty dataframe to confirm that the columns are as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's loop through the data and fill the dataframe one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for data in neighborhoods_data:\n",
    "    borough = neighborhood_name = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly examine the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighborhoods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure that the dataset has all 5 boroughs and 306 neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
    "        len(neighborhoods['Borough'].unique()),\n",
    "        neighborhoods.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use geopy library to get the latitude and longitude values of Manhattan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent <em>ny_explorer</em>, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a map of Manhattan with neighborhoods superimposed on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company has chosen to relocate into a neighborhood in Manhattan. So let's slice the original dataframe and create a new dataframe of the Manhattan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_data = neighborhoods[neighborhoods['Borough'] == 'Manhattan'].reset_index(drop=True)\n",
    "manhattan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the geographical coordinates of Manhattan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "address = 'Manhattan, NY'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ny_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Manhattan are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with all of New York City, let's visualizat Manhattan the neighborhoods in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create map of Manhattan using latitude and longitude values\n",
    "map_manhattan = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, label in zip(manhattan_data['Latitude'], manhattan_data['Longitude'], manhattan_data['Neighborhood']):\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_manhattan)  \n",
    "    \n",
    "map_manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to start utilizing the Foursquare API to explore the neighborhoods and segment them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CLIENT_ID = 'FD0EOCRMLZBUH0QSGNF0NSNXMPAMT2I5OLGUSGQSAVMDCDZH' # your Foursquare ID\n",
    "CLIENT_SECRET = 'QXC0KBN1WHQ0I40KKACAPQRC2OS1EBKOWCE0LWBSUJD1IDMS' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore the first neighborhood in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_data.loc[0, 'Neighborhood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "neighborhood_latitude = manhattan_data.loc[0, 'Latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = manhattan_data.loc[0, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "neighborhood_name = manhattan_data.loc[0, 'Neighborhood'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get the top 20 venues that are in Marble Hill within a radius of 200 meters (roughly 1 city block). By focusing on items nearest to the relocation center we will demonstrate to employees we valued their survey responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the GET request URL. Name your URL **url**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type your answer here\n",
    "\n",
    "LIMIT = 20 # limit of number of venues returned by Foursquare API\n",
    "\n",
    "radius = 200 # define radius\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "url # display URL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the GET request and examine the resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = requests.get(url).json()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Foursquare lab in the previous module, we know that all the information is in the *items* key. Before we proceed, let's borrow the **get_category_type** function from the Foursquare lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to clean the json and structure it into a *pandas* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many venues were returned by Foursquare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Neighborhoods in Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a function to repeat the same process to all the neighborhoods in Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now write the code to run the above function on each neighborhood and create a new dataframe called *manhattan_venues*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type your answer here\n",
    "\n",
    "manhattan_venues = getNearbyVenues(names=manhattan_data['Neighborhood'],\n",
    "                                   latitudes=manhattan_data['Latitude'],\n",
    "                                   longitudes=manhattan_data['Longitude']\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "manhattan_venues = getNearbyVenues(names=manhattan_data['Neighborhood'],\n",
    "                                   latitudes=manhattan_data['Latitude'],\n",
    "                                   longitudes=manhattan_data['Longitude']\n",
    "                                  )\n",
    "--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the size of the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(manhattan_venues.shape)\n",
    "manhattan_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the Venue Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manhattan_venues[['Venue Category']].groupby(['Venue Category']).size().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we need to map 'Venue Category' to 'Factors' that the employees ranked for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map all resturants and bars to \n",
    "manhattan_venues[['Venue Category']].groupby(['Venue Category']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.narr1.str.contains(\"F[AE]LL.*BI[KC]\")]\n",
    "#manhattan_venues['Venue Category'].str.contains(\"Restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing re package for using regular expressions \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map venue categories to employee factors \n",
    "def Map_Venue_Category(Venue_Category): \n",
    "    \n",
    "    #print (Venue_Category)\n",
    "    if re.search('Restaurant', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant' \n",
    "    \n",
    "    if re.search('Joint', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant' \n",
    "    \n",
    "    if re.search('Tea Room', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant' \n",
    "    \n",
    "    if re.search('Pizza Place', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant' \n",
    "    \n",
    "    if re.search('Donut Shop', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant' \n",
    "    \n",
    "    if re.search('Coffee Shop', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Bagel Shop', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Bakery', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Bar', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Bistro', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Beer', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Burrito', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Breakfast', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Tea Shop', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Candy', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Caf√©', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Cheese', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Chocolate', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Bridge', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'MassTransit'\n",
    "    \n",
    "    if re.search('Bus Line', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'MassTransit'\n",
    "    \n",
    "    if re.search('Community Center', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Cupcake', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Deli', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Dessert', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Diner', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Dog Run', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Farmers Market', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Food', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Fountain', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Yogurt', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Gourmet', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Grocery', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Gym', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Gym'\n",
    "    \n",
    "    if re.search('Harbor', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Histor', Venue_Category, re.IGNORECASE): #for historic and history\n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('ice cream', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Liquor', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Memorial', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Pie', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Poke Place', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Pool', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Snack', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Steak', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Taco', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Tennis Stadium', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Trail', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Field', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Spa', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Gym'\n",
    "    \n",
    "    if re.search('Golf Course', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Playground', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Plaza', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Basketball Court', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('GastroPub', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Momument', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Sandwich', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Gastro', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Restaurant'\n",
    "    \n",
    "    if re.search('Tennis', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    if re.search('Waterfront', Venue_Category, re.IGNORECASE): \n",
    "        \n",
    "        return 'Park'\n",
    "    \n",
    "    else: \n",
    "        # if clean up needed return the same name \n",
    "        return Venue_Category \n",
    "          \n",
    "# Set the Employee Venue Category\n",
    "manhattan_venues['Employee_Venue_Category'] = manhattan_venues['Venue Category'].apply(Map_Venue_Category)\n",
    "  \n",
    "# Print the updated dataframe \n",
    "manhattan_venues[['Employee_Venue_Category']].groupby(['Employee_Venue_Category']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the venues were returned for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_venues[['Neighborhood','Employee_Venue_Category']].groupby(['Neighborhood','Employee_Venue_Category']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find out how many unique categories can be curated from all the returned venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('There are {} unique employee venue categories.'.format(len(manhattan_venues['Employee_Venue_Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Each Neighborhood by Employee Mapped Venue Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "manhattan_onehot = pd.get_dummies(manhattan_venues[['Employee_Venue_Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "manhattan_onehot['Neighborhood'] = manhattan_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [manhattan_onehot.columns[-1]] + list(manhattan_onehot.columns[:-1])\n",
    "manhattan_onehot = manhattan_onehot[fixed_columns]\n",
    "\n",
    "manhattan_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's examine the new dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's group rows by neighborhood and by taking the mean of the frequency of occurrence of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_grouped = manhattan_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "manhattan_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's confirm the new size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_grouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print each neighborhood along with the top 5 most common venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in manhattan_grouped['Neighborhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = manhattan_grouped[manhattan_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's put that into a *pandas* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to sort the venues in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the new dataframe and display the top 10 venues for each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = manhattan_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(manhattan_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(manhattan_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run *k*-means to cluster the neighborhood into 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "manhattan_grouped_clustering = manhattan_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(manhattan_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "manhattan_merged = manhattan_data\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "manhattan_merged = manhattan_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "manhattan_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the resulting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(manhattan_merged['Latitude'], manhattan_merged['Longitude'], manhattan_merged['Neighborhood'], manhattan_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can examine each cluster and determine the discriminating venue categories that distinguish each cluster. Based on the defining categories, you can then assign a name to each cluster. I will leave this exercise to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 0, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 1, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 2, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 3, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged['Cluster Labels'] == 4, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Find the location that has all of the venues in the rank order chosen by the employees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_merged.loc[(manhattan_merged['1st Most Common Venue'] == 'Restaurant') & \n",
    "                     (manhattan_merged['2nd Most Common Venue'] == 'Gym') & \n",
    "                     (manhattan_merged['3rd Most Common Venue'] == 'Park')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccommendation based on results - Carnegie Hill \n",
    "Based on the analysis we found that the Neighborhood of Carnegie Hill has all the venues in rank order that the employees of the startup selected in their survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key element of this analysis was clearly the ranking of factors by employees. This was by design as the HR Director wanted to ensure employees felt they were heard and respected in this office relocations.  As a startup it is critical to maintain key staff members.  The analysis that the Data Science team undertook found the one location in Manhattan, NY that met all criteria.\n",
    "\n",
    "Potentially, the analysis could be tweaked by adding new survey questions for employees or weighting the response of developers and data scientist over others in the company. Also, the search radius around each location could be increased from 200 meters (1 city block) to 500 meters(2.5 city blocks) to get more information on nearby venues.  However, that could lead to noise and potentially overlapping locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis met its desired outcome by identifying the single location in Manhattan that clear matched all the ranked desires of the staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
